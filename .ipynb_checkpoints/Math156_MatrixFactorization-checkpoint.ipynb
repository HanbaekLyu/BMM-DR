{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network for MNIST image classficiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.utils.extmath import softmax\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from tqdm import trange\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_loading(data_shape=[10,10,10], n_components=5):\n",
    "    ### loading = python dict of [U1, U2, \\cdots, Un], each Ui is I_i x R array\n",
    "    loading = {}\n",
    "    for i in np.arange(len(data_shape)):  # n_modes = X.ndim -1 where -1 for the last `batch mode'\n",
    "        loading.update({'U' + str(i): np.random.rand(data_shape[i], n_components)})\n",
    "    return loading\n",
    "\n",
    "def out(loading, drop_last_mode=False):\n",
    "    ### given loading, take outer product of respected columns to get CPdict\n",
    "    ### Use drop_last_mode for ALS\n",
    "    CPdict = {}\n",
    "    n_components = loading.get(\"U0\").shape[1]\n",
    "    for i in np.arange(n_components):\n",
    "        A = np.array([1])\n",
    "        if drop_last_mode:\n",
    "            n_modes_multiplied = len(loading.keys()) - 1\n",
    "        else:\n",
    "            n_modes_multiplied = len(loading.keys())  # also equals self.X_dim - 1\n",
    "        for j in np.arange(n_modes_multiplied):\n",
    "            loading_factor = loading.get('U' + str(j))  ### I_i X n_components matrix\n",
    "            # print('loading_factor', loading_factor)\n",
    "            A = np.multiply.outer(A, loading_factor[:, i])\n",
    "        A = A[0]\n",
    "        CPdict.update({'A' + str(i): A})\n",
    "    return CPdict\n",
    "\n",
    "def Out_tensor(loading):\n",
    "    ### given loading, take outer product of respected columns to get CPdict\n",
    "    CPdict = out(loading, drop_last_mode=False)\n",
    "    recons = np.zeros(CPdict.get('A0').shape)\n",
    "    for j in np.arange(len(CPdict.keys())):\n",
    "        recons += CPdict.get('A' + str(j))\n",
    "\n",
    "    return recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "loading = {}\n",
    "\n",
    "np.random.seed(1)\n",
    "U0 = np.random.rand(5, n_components)\n",
    "np.random.seed(2)\n",
    "U1 = np.random.rand(5, n_components)\n",
    "#np.random.seed(3)\n",
    "#U2 = np.random.rand(300, n_components)\n",
    "\n",
    "loading.update({'U0': U0})\n",
    "loading.update({'U1': U1})\n",
    "#loading.update({'U2': U2})\n",
    "\n",
    "X = Out_tensor(loading) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recons_error1(data, loading):\n",
    "    CPdict = out(loading, drop_last_mode=False)\n",
    "    recons = np.zeros(data.shape)\n",
    "    for j in np.arange(len(CPdict.keys())):\n",
    "        recons += CPdict.get('A' + str(j))\n",
    "    error = np.linalg.norm((data - recons).reshape(-1, 1), ord=2)\n",
    "    error /= np.linalg.norm(data)\n",
    "    return recons \n",
    "\n",
    "def recons_error2(data, loading):\n",
    "    n_components = loading.get(\"U0\").shape[1]\n",
    "    X_new_mat = data.reshape(-1, data.shape[-1])\n",
    "    print('X_new_mat.shape', X_new_mat.shape)\n",
    "    \n",
    "    CPdict = out(loading, drop_last_mode=True)\n",
    "    \n",
    "    H = loading.get('U' + str(len(loading.keys())-1))\n",
    "    print('len(loading.keys())-1', len(loading.keys())-1)\n",
    "    W = np.zeros(shape=(X_new_mat.shape[0], n_components))\n",
    "    for j in np.arange(n_components):\n",
    "        W[:, j] = CPdict.get('A' + str(j)).reshape(-1, 1)[:, 0]\n",
    "\n",
    "    print('X_new_mat.shape', X_new_mat.shape)\n",
    "    print('W.shape', W.shape)\n",
    "    print('H.shape', H.shape)\n",
    "        \n",
    "    error = np.linalg.norm(X_new_mat - W @ H.T)\n",
    "    error /= np.linalg.norm(X_new_mat)\n",
    "    return W @ H.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares for Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_code_within_radius(X, W, H0=None, r=None, a1=0, a2=0,\n",
    "                              sub_iter=[5], stopping_grad_ratio=0.02,\n",
    "                              subsample_ratio=None, nonnegativity=True,\n",
    "                              sparsity_on_colums=False):\n",
    "    '''\n",
    "    Find \\hat{H} = argmin_H ( || X - WH||^2 + alpha|H| ) within radius r from H0\n",
    "    Use row-wise projected gradient descent\n",
    "    Do NOT sparsecode the whole thing and then project -- instable\n",
    "    12/5/2020 Lyu\n",
    "\n",
    "    For NTF problems, X is usually tall and thin so it is better to subsample from rows\n",
    "    12/25/2020 Lyu\n",
    "\n",
    "    Apply single round of AdaGrad for rows, stop when gradient norm is small and do not make update\n",
    "    12/27/2020 Lyu\n",
    "    '''\n",
    "    # print('!!!! H0.shape', H0.shape)\n",
    "\n",
    "    if H0 is None:\n",
    "        H0 = np.random.rand(W.shape[1], X.shape[1])\n",
    "\n",
    "    H1 = H0.copy()\n",
    "    i = 0\n",
    "    dist = 1\n",
    "    mode = 0\n",
    "\n",
    "    if sparsity_on_colums:\n",
    "        mode = 1\n",
    "\n",
    "    idx = np.arange(X.shape[mode])\n",
    "    # print('!!! X.shape', X.shape)\n",
    "\n",
    "    if (subsample_ratio is not None) and (X.shape[0]>X.shape[1]):\n",
    "        idx = np.random.randint(X.shape[0], size=X.shape[0]//subsample_ratio)\n",
    "        A = W[idx,:].T @ W[idx,:]\n",
    "        B = W[idx,:].T @ X[idx,:]\n",
    "        H_red = H1\n",
    "\n",
    "    elif (subsample_ratio is not None) and (X.shape[0]<= X.shape[1]):\n",
    "        idx = np.random.randint(X.shape[1], size=X.shape[1]//subsample_ratio)\n",
    "        A = W[:,:].T @ W[:,:]\n",
    "        B = W[:,:].T @ X[:,idx]\n",
    "        H_red = H1[:,idx]\n",
    "\n",
    "    else:\n",
    "        A = W[:,:].T @ W[:,:]\n",
    "        B = W[:,:].T @ X[:,:]\n",
    "        H_red = H1\n",
    "\n",
    "    while (i < np.random.choice(sub_iter)):\n",
    "        if_continue = np.ones(H0.shape[mode])  # indexed by rows of H\n",
    "        H1_old = H1.copy()\n",
    "        b = 10\n",
    "        if mode==0:\n",
    "            for k in [k for k in np.arange(H0.shape[0]) if if_continue[k]>0.5]:\n",
    "                # row-wise gradient descent\n",
    "                grad = (np.dot(A[k, :], H_red) - B[k, :] + a1 * np.ones(B.shape[1]) + a2 * H_red[k,:] )\n",
    "                grad_norm = np.linalg.norm(grad, 2)\n",
    "                step_size = (1 / (((i + 2) ** (1)) * (A[k, k] + 1)))\n",
    "                if r is not None:  # usual sparse coding without radius restriction\n",
    "                    d = step_size * grad_norm\n",
    "                    step_size = (r / max(r, d)) * step_size\n",
    "\n",
    "                if step_size * grad_norm / np.linalg.norm(H1_old, 2) > stopping_grad_ratio:\n",
    "                    if (subsample_ratio is not None) and (X.shape[0]<= X.shape[1]):\n",
    "                        H1[k, idx] = H1[k, idx] - step_size * grad\n",
    "                    else:\n",
    "                        H1[k, :] = H1[k, :] - step_size * grad\n",
    "                else:\n",
    "                    if_continue[k] = 0  # stop making changes when negligible\n",
    "                    # print('!!! update skipped' )\n",
    "\n",
    "                # print('!!! H1.shape', H1.shap\n",
    "                if nonnegativity:\n",
    "                    H1[k,:] = np.maximum(H1[k,:], np.zeros(shape=(H1.shape[1],)))  # nonnegativity constraint\n",
    "\n",
    "        else:\n",
    "            for k in [k for k in np.arange(H0.shape[1]) if if_continue[k]>0.5]:\n",
    "                # column-wise gradient descent\n",
    "                grad = (np.dot(A[:, :], H1[:,k]) - B[:, k] + a1 * np.ones(H0.shape[0])) + a2 * H1[:,k]\n",
    "                grad_norm = np.linalg.norm(grad, 2)\n",
    "                step_size = (1 / (((i + 2) ** (1)) * (A[k, k] + 1)))\n",
    "                if r is not None:  # usual sparse coding without radius restriction\n",
    "                    d = step_size * grad_norm\n",
    "                    step_size = (r / max(r, d)) * step_size\n",
    "\n",
    "                if step_size * grad_norm / np.linalg.norm(H1_old, 2) > stopping_grad_ratio:\n",
    "                    H1[:, k] = H1[:, k] - step_size * grad\n",
    "                else:\n",
    "                    if_continue[k] = 0  # stop making changes when negligible\n",
    "                    # print('!!! update skipped' )\n",
    "\n",
    "                # print('!!! H1.shape', H1.shap\n",
    "                if nonnegativity:\n",
    "                    H1[:,k] = np.maximum(H1[:,k], np.zeros(shape=(H1.shape[0],)))  # nonnegativity constraint\n",
    "\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    return H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_code_within_radius_old(X, W, H0, r=None, a1=0,\n",
    "                                  sub_iter=[2], stopping_grad_ratio=0.0001,\n",
    "                                  subsample_ratio=None, nonnegativity=True,\n",
    "                                  use_line_search=False):\n",
    "        '''\n",
    "        Find \\hat{H} = argmin_H ( | X - WH| + alpha|H| ) within radius r from H0\n",
    "        Use row-wise projected gradient descent\n",
    "        Do NOT sparsecode the whole thing and then project -- instable\n",
    "        12/5/2020 Lyu\n",
    "\n",
    "        For NTF problems, X is usually tall and thin so it is better to subsample from rows\n",
    "        12/25/2020 Lyu\n",
    "\n",
    "        Apply single round of AdaGrad for rows, stop when gradient norm is small and do not make update\n",
    "        12/27/2020 Lyu\n",
    "        '''\n",
    "\n",
    "        # print('!!!! H0.shape', H0.shape)\n",
    "        #if H0 is None:\n",
    "        #    H0 = np.random.rand(W.shape[1], X.shape[1])\n",
    "        H1 = H0.copy()\n",
    "        i = 0\n",
    "        dist = 1\n",
    "        idx = np.arange(X.shape[0])\n",
    "        H1_old = H1.copy()\n",
    "        \n",
    "        if (subsample_ratio is not None):\n",
    "            idx = np.random.randint(X.shape[1], size=X.shape[1]//subsample_ratio)\n",
    "            A = W[:,:].T @ W[:,:]\n",
    "            B = W[:,:].T @ X[:,idx]\n",
    "            print('len(set(idx))', len(set(idx)))\n",
    "        else:\n",
    "            A = W[:,:].T @ W[:,:]\n",
    "            B = W[:,:].T @ X[:,:]\n",
    "\n",
    "        while (i < np.random.choice(sub_iter)):\n",
    "            if_continue = np.ones(H0.shape[0])  # indexed by rows of H\n",
    "\n",
    "            for k in [k for k in np.arange(H0.shape[0]) if if_continue[k]>0.5]:\n",
    "\n",
    "                grad = (np.dot(A[k, :], H1) - B[k, :] + a1 * np.sign(H1[k, :]) * np.ones(H0.shape[1]))\n",
    "                grad_norm = np.linalg.norm(grad, 2)\n",
    "\n",
    "                # Initial step size\n",
    "                step_size = 1/(A[k,k]+1)\n",
    "                # step_size = 1 / (np.trace(A)) # use the whole trace\n",
    "                # step_size = 1\n",
    "                if r is not None:  # usual sparse coding without radius restriction\n",
    "                    d = step_size * grad_norm\n",
    "                    step_size = (r / max(r, d)) * step_size\n",
    "\n",
    "                H1_temp = H1.copy()\n",
    "                # loss_old = np.linalg.norm(X - W @ H1)**2\n",
    "                H1_temp[k, :] = H1[k, :] - step_size * grad\n",
    "                if nonnegativity:\n",
    "                    H1_temp[k,:] = np.maximum(H1_temp[k,:], np.zeros(shape=(H1.shape[1],)))  # nonnegativity constraint\n",
    "                #loss_new = np.linalg.norm(X - W @ H1_temp)**2\n",
    "                #if loss_old > loss_new:\n",
    "                H1 = H1_temp\n",
    "                    # print('recons_loss:' , np.linalg.norm(X - W @ H1, ord=2) / np.linalg.norm(X, ord=2))\n",
    "                \"\"\"\n",
    "                # Armijo backtraking line search\n",
    "                m = grad.T @ H1[k,:]\n",
    "                H1_temp = H1.copy()\n",
    "                loss_old = np.linalg.norm(X - W @ H1)**2\n",
    "                loss_new = 0\n",
    "                count = 0\n",
    "                while (count==0) or (loss_old - loss_new < 0.1 * step_size * m):\n",
    "                    step_size /= 2\n",
    "                    H1_temp[k, :] = H1[k, :] - step_size * grad\n",
    "                    if nonnegativity:\n",
    "                        H1_temp[k,:] = np.maximum(H1_temp[k,:], np.zeros(shape=(H1.shape[1],)))  # nonnegativity constraint\n",
    "                    loss_new = np.linalg.norm(X - W @ H1_temp)**2\n",
    "                    count += 1\n",
    "                    # print('--- loss_old - loss_new', loss_old - loss_new)\n",
    "                    # print('line search step size:', step_size)\n",
    "                    #print('count == ', count)\n",
    "                    #print('recons_loss:' , np.linalg.norm(X - W @ H1_temp, ord=2) / np.linalg.norm(X, ord=2))\n",
    "                    #print('recons loss tensor', self.compute_recons_error(data=self.X, loading=self.loading))\n",
    "                \"\"\"\n",
    "\n",
    "                # H1 = H1_temp\n",
    "\n",
    "\n",
    "            # dist = np.linalg.norm(H1 - H1_old, 2) / np.linalg.norm(H1_old, 2)\n",
    "            # print('!!! dist', dist)\n",
    "            # H1_old = H1\n",
    "            i = i + 1\n",
    "            # print('!!!! i', i)  # mostly the loop finishes at i=1 except the first round\n",
    "\n",
    "\n",
    "        return H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coding(X, W, H0, \n",
    "          r=None, \n",
    "          a1=0, #L1 regularizer\n",
    "          a2=0, #L2 regularizer\n",
    "          sub_iter=[5], \n",
    "          stopping_grad_ratio=0.0001, \n",
    "          nonnegativity=True,\n",
    "          subsample_ratio=1):\n",
    "    \"\"\"\n",
    "    Find \\hat{H} = argmin_H ( || X - WH||_{F}^2 + a1*|H| + a2*|H|_{F}^{2} ) within radius r from H0\n",
    "    Use row-wise projected gradient descent\n",
    "    \"\"\"\n",
    "    H1 = H0.copy()\n",
    "    i = 0\n",
    "    dist = 1\n",
    "    idx = np.arange(X.shape[1])\n",
    "    if subsample_ratio>1:  # subsample columns of X and solve reduced problem (like in SGD)\n",
    "        idx = np.random.randint(X.shape[1], size=X.shape[1]//subsample_ratio)\n",
    "    A = W.T @ W ## Needed for gradient computation\n",
    "    grad = W.T @ (W @ H0 - X)\n",
    "    while (i < np.random.choice(sub_iter)):\n",
    "        step_size = (1 / (((i + 1) ** (1)) * (np.trace(A) + 1)))\n",
    "        H1 -= step_size * grad \n",
    "        if nonnegativity:\n",
    "            H1 = np.maximum(H1, 0)  # nonnegativity constraint\n",
    "        i = i + 1\n",
    "    return H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(X,\n",
    "        n_components = 10, # number of columns in the dictionary matrix W\n",
    "        n_iter=100,\n",
    "        a0 = 0, # L1 regularizer for H\n",
    "        a1 = 0, # L1 regularizer for W\n",
    "        a12 = 0, # L2 regularizer for W\n",
    "        H_nonnegativity=True,\n",
    "        W_nonnegativity=True,\n",
    "        compute_recons_error=False,\n",
    "        subsample_ratio = 1):\n",
    "    \n",
    "        '''\n",
    "        Given data matrix X, use alternating least squares to find factors W,H so that \n",
    "                                || X - WH ||_{F}^2 + a0*|H|_{1} + a1*|W|_{1} + a12 * |W|_{F}^{2}\n",
    "        is minimized (at least locally)\n",
    "        '''\n",
    "        \n",
    "        d, n = X.shape\n",
    "        r = n_components\n",
    "        \n",
    "        #normalization = np.linalg.norm(X.reshape(-1,1),1)/np.product(X.shape) # avg entry of X\n",
    "        #print('!!! avg entry of X', normalization)\n",
    "        #X = X/normalization\n",
    "\n",
    "        # Initialize factors \n",
    "        W = np.random.rand(d,r)\n",
    "        H = np.random.rand(r,n) \n",
    "        # H = H * np.linalg.norm(X) / np.linalg.norm(H)\n",
    "        for i in trange(n_iter):\n",
    "            H = update_code_within_radius_old(X=X, W=W.copy(), H0=H.copy(), r=None, a1=a0, nonnegativity=H_nonnegativity, subsample_ratio=subsample_ratio)\n",
    "            W = update_code_within_radius_old(X=X.T, W=H.copy().T, H0=W.copy().T, r=None, a1=a1, nonnegativity=W_nonnegativity, subsample_ratio=subsample_ratio).T\n",
    "            #H = coding(X, W.copy(), H.copy(), a1=a0, nonnegativity=H_nonnegativity, subsample_ratio=subsample_ratio)\n",
    "            #W = coding(X.T, H.copy().T, W.copy().T, a1=a1, a2=a12, nonnegativity=W_nonnegativity, subsample_ratio=subsample_ratio).T\n",
    "            W /= np.linalg.norm(W)\n",
    "            if compute_recons_error and (i % 10 == 0) :\n",
    "                print('iteration %i, reconstruction error %f' % (i, np.linalg.norm(X-W@H)**2))\n",
    "        return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 971.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 0, reconstruction error 198.149292\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 10, reconstruction error 104.745784\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 20, reconstruction error 76.033956\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 30, reconstruction error 123.770835\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 40, reconstruction error 96.042604\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 50, reconstruction error 103.964760\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 60, reconstruction error 125.071495\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 70, reconstruction error 77.150577\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 80, reconstruction error 75.798181\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "iteration 90, reconstruction error 70.406541\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "subsample_ratio 1\n",
      "reconstruction error (relative) = 0.245093\n",
      "Dictionary error (relative) = 0.910645\n",
      "Code error (relative) = 6.377299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulated Data and its factorization \n",
    "\n",
    "W0 = np.random.rand(10,5)\n",
    "H0 = np.random.rand(5,20)\n",
    "X0 = W0 @ H0\n",
    "\n",
    "W, H = ALS(X=X0, \n",
    "           n_components=5, \n",
    "           n_iter=100, \n",
    "           a0 = 0, # L1 regularizer for H\n",
    "           a1 = 1, # L1 regularizer for W\n",
    "           a12 = 0, # L2 regularizer for W\n",
    "           H_nonnegativity=True,\n",
    "           W_nonnegativity=True,\n",
    "           compute_recons_error=True,\n",
    "           subsample_ratio=1)\n",
    "\n",
    "print('reconstruction error (relative) = %f' % (np.linalg.norm(X0-W@H)**2/np.linalg.norm(X0)**2))\n",
    "print('Dictionary error (relative) = %f' % (np.linalg.norm(W0 - W)**2/np.linalg.norm(W0)**2))\n",
    "print('Code error (relative) = %f' % (np.linalg.norm(H0-H)**2/np.linalg.norm(H0)**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn dictionary of MNIST images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dictionary(W, save_name=None, score=None, grid_shape=None):\n",
    "    k = int(np.sqrt(W.shape[0]))\n",
    "    rows = int(np.sqrt(W.shape[1]))\n",
    "    cols = int(np.sqrt(W.shape[1]))\n",
    "    if grid_shape is not None:\n",
    "        rows = grid_shape[0]\n",
    "        cols = grid_shape[1]\n",
    "    \n",
    "    figsize0=(6, 6)\n",
    "    if (score is None) and (grid_shape is not None):\n",
    "       figsize0=(cols, rows)\n",
    "    if (score is not None) and (grid_shape is not None):\n",
    "       figsize0=(cols, rows+0.2)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=rows, ncols=cols, figsize=figsize0,\n",
    "                            subplot_kw={'xticks': [], 'yticks': []})\n",
    "        \n",
    "        \n",
    "    for ax, i in zip(axs.flat, range(100)):\n",
    "        if score is not None:\n",
    "            idx = np.argsort(score)\n",
    "            idx = np.flip(idx)    \n",
    "            \n",
    "            ax.imshow(W.T[idx[i]].reshape(k, k), cmap=\"viridis\", interpolation='nearest')\n",
    "            ax.set_xlabel('%1.2f' % score[i], fontsize=13)  # get the largest first\n",
    "            ax.xaxis.set_label_coords(0.5, -0.05)\n",
    "        else: \n",
    "            ax.imshow(W.T[i].reshape(k, k), cmap=\"viridis\", interpolation='nearest')\n",
    "            if score is not None:\n",
    "                ax.set_xlabel('%1.2f' % score[i], fontsize=13)  # get the largest first\n",
    "                ax.xaxis.set_label_coords(0.5, -0.05)\n",
    "       \n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle('Dictionary learned from patches of size %d' % k, fontsize=16)\n",
    "    plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "    \n",
    "    if save_name is not None:\n",
    "        plt.savefig( save_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dictionary_list(W_list, label_list, save_name=None, score_list=None):\n",
    "    # Make plot\n",
    "    \n",
    "    # outer gridspec\n",
    "    nrows=1\n",
    "    ncols=len(W_list)\n",
    "    fig = plt.figure(figsize=(16, 5), constrained_layout=False)\n",
    "    outer_grid = gridspec.GridSpec(nrows=nrows, ncols=ncols, wspace=0.1, hspace=0.05)\n",
    "\n",
    "    \n",
    "    \n",
    "    # make nested gridspecs\n",
    "    for i in range(1 * ncols):\n",
    "        k = int(np.sqrt(W_list[i].shape[0]))\n",
    "        sub_rows = int(np.sqrt(W_list[i].shape[1]))\n",
    "        sub_cols = int(np.sqrt(W_list[i].shape[1]))\n",
    "\n",
    "        idx = np.arange(W_list[i].shape[1])\n",
    "        if score_list is not None:\n",
    "            idx = np.argsort(score_list[i])\n",
    "            idx = np.flip(idx)    \n",
    "        \n",
    "        inner_grid = outer_grid[i].subgridspec(sub_rows, sub_cols, wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for j in range(sub_rows*sub_cols):\n",
    "            a = j // sub_cols \n",
    "            b = j % sub_cols #sub-lattice indices\n",
    "\n",
    "            ax = fig.add_subplot(inner_grid[a, b])\n",
    "            ax.imshow(W_list[i].T[idx[j]].reshape(k, k), cmap=\"viridis\", interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            if (b>0):\n",
    "                ax.set_yticks([])\n",
    "            if (a < sub_rows-1):\n",
    "                ax.set_xticks([])\n",
    "            if (a == 0) and (b==2):\n",
    "                #ax.set_title(\"W_nonnegativity$=$ %s \\n H_nonnegativity$=$ %s\" \n",
    "                #             % (str(nonnegativity_list[i][0]), str(nonnegativity_list[i][1])), y=1.2, fontsize=14) \n",
    "                ax.set_title(label_list[i], y=1.2, fontsize=14)\n",
    "            if (score_list is not None) and (score_list[i] is not None):\n",
    "                ax.set_xlabel('%1.2f' % score_list[i][idx[j]], fontsize=13)  # get the largest first\n",
    "                ax.xaxis.set_label_coords(0.5, -0.07)\n",
    "           \n",
    "                \n",
    "                \n",
    "    # plt.suptitle('Dictionary learned from patches of size %d' % k, fontsize=16)\n",
    "    plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "    plt.savefig(save_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "# X = X.values  ### Uncomment this line if you are having type errors in plotting. It is loading as a pandas dataframe, but our indexing is for numpy array. \n",
    "X = X / 255.\n",
    "\n",
    "print('X.shape', X.shape)\n",
    "print('y.shape', y.shape)\n",
    "\n",
    "'''\n",
    "Each row of X is a vectroization of an image of 28 x 28 = 784 pixels.  \n",
    "The corresponding row of y holds the true class label from {0,1, .. , 9}.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained matrix factorization and dictionary images \n",
    "\n",
    "idx = np.random.choice(np.arange(X.shape[1]), 100)\n",
    "\n",
    "X0 = X[idx,:].T\n",
    "\n",
    "W, H = ALS(X=X0, \n",
    "           n_components=25, \n",
    "           n_iter=50, \n",
    "           subsample_ratio=1, \n",
    "           W_nonnegativity=False,\n",
    "           H_nonnegativity=False,\n",
    "           compute_recons_error=True)\n",
    "\n",
    "display_dictionary(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and dictionary images (principal components)\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "pca.fit(X)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "\n",
    "display_dictionary(W, score=s, save_name = \"MNIST_PCA_ex1.pdf\", grid_shape=[1,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(X.shape[1]), 100)\n",
    "\n",
    "X0 = X[idx,:].T\n",
    "n_iter = 10\n",
    "W_list = []\n",
    "\n",
    "nonnegativitiy = [[False, False], [False, True], [True, True]]\n",
    "\n",
    "for i in np.arange(3): \n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    \n",
    "\n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, label_list = label_list, save_name = \"MNIST_NMF_ex1.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF and PCA on MNIST\n",
    "\n",
    "idx = np.random.choice(np.arange(X.shape[1]), 100)\n",
    "\n",
    "X0 = X[idx,:].T\n",
    "n_iter = 100\n",
    "W_list = []\n",
    "H_list = []\n",
    "\n",
    "nonnegativitiy = ['PCA', [False, False], [False, True], [True, True]]\n",
    "\n",
    "#PCA \n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "W_list.append(W)\n",
    "H_list.append(s)\n",
    "\n",
    "# MF\n",
    "for i in np.arange(1,len(nonnegativitiy)): \n",
    "    print('!!! nonnegativitiy[i]', nonnegativitiy[i])\n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    H_list.append(H)\n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        label = nonnegativitiy[0]\n",
    "    else:\n",
    "        label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "score_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        score_list.append(H_list[0])\n",
    "    else:\n",
    "        H = H_list[i]\n",
    "        score = np.sum(abs(H), axis=1) # sum of the coefficients of each columns of W = overall usage \n",
    "        score_list.append(score)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, \n",
    "                        label_list = label_list, \n",
    "                        score_list = score_list,\n",
    "                        save_name = \"MNIST_PCA_NMF_ex1.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_padding(img, thickness=1):\n",
    "    # img = a x b image \n",
    "    [a,b] = img.shape\n",
    "    Y = np.zeros(shape=[a+thickness, b+thickness])\n",
    "    r_loc = np.random.choice(np.arange(thickness+1))\n",
    "    c_loc = np.random.choice(np.arange(thickness+1))\n",
    "    Y[r_loc:r_loc+a, c_loc:c_loc+b] = img\n",
    "    return Y\n",
    "\n",
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y\n",
    "\n",
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "\n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiclass_MNIST_padding(list_digits=['0','1', '2'], full_MNIST=[X,y], padding_thickness=10):\n",
    "    # get train and test set from MNIST of given digits\n",
    "    # e.g., list_digits = ['0', '1', '2']\n",
    "    # pad each 28 x 28 image with zeros so that it has now \"padding_thickness\" more rows and columns\n",
    "    # The original image is superimposed at a uniformly chosen location \n",
    "    if full_MNIST is not None:\n",
    "        X, y = full_MNIST\n",
    "    else:\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "        X = X / 255.\n",
    "    Y = list2onehot(y.tolist(), list_digits)\n",
    "    \n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_digits] # list of indices where the label y is in list_digits\n",
    "    \n",
    "    X01 = X[idx,:]\n",
    "    y01 = Y[idx,:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "    y_train = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "\n",
    "    for i in trange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        img_padded = random_padding(X01[i,:].reshape(28,28), thickness=padding_thickness)\n",
    "        img_padded_vec = img_padded.reshape(1,-1)\n",
    "        if U<0.8:\n",
    "            X_train.append(img_padded_vec[0,:].copy())\n",
    "            y_train.append(y01[i,:].copy())\n",
    "        else:\n",
    "            X_test.append(img_padded_vec[0,:].copy())\n",
    "            y_test.append(y01[i,:].copy())\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MNIST binary classification experiments \n",
    "\n",
    "list_digits=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST_padding(list_digits=list_digits, \n",
    "                                                                   full_MNIST=[X,y],\n",
    "                                                                   padding_thickness=20)\n",
    "\n",
    "\n",
    "idx = np.random.choice(np.arange(X_train.shape[1]), 100)\n",
    "X0 = X_train[idx,:].T\n",
    "\n",
    "n_iter = 100\n",
    "W_list = []\n",
    "\n",
    "nonnegativitiy = [[False, False], [False, True], [True, True]]\n",
    "\n",
    "for i in np.arange(3): \n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    \n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, label_list = label_list, save_name = \"MNIST_NMF_ex2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF and PCA on MNIST + padding\n",
    "\n",
    "list_digits=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "X_train, X_test, y_train, y_test = sample_multiclass_MNIST_padding(list_digits=list_digits, \n",
    "                                                                   full_MNIST=[X,y],\n",
    "                                                                   padding_thickness=20)\n",
    "\n",
    "\n",
    "\n",
    "idx = np.random.choice(np.arange(X.shape[1]), 100)\n",
    "\n",
    "X0 = X_train[idx,:].T\n",
    "n_iter = 100\n",
    "W_list = []\n",
    "H_list = []\n",
    "\n",
    "nonnegativitiy = ['PCA', [False, False], [False, True], [True, True]]\n",
    "\n",
    "#PCA \n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "W_list.append(W)\n",
    "H_list.append(s)\n",
    "\n",
    "# MF\n",
    "for i in np.arange(1,len(nonnegativitiy)): \n",
    "    print('!!! nonnegativitiy[i]', nonnegativitiy[i])\n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    H_list.append(H)\n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        label = nonnegativitiy[0]\n",
    "    else:\n",
    "        label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "score_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        score_list.append(H_list[0])\n",
    "    else:\n",
    "        H = H_list[i]\n",
    "        score = np.sum(abs(H), axis=1) # sum of the coefficients of each columns of W = overall usage \n",
    "        score_list.append(score)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, \n",
    "                        label_list = label_list, \n",
    "                        score_list = score_list,\n",
    "                        save_name = \"MNIST_PCA_NMF_ex2.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Learing for Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True,\n",
    "                                random_state=np.random.seed(0))\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "#faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "#faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "print(\"faces_centered.shape\", faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample images \n",
    "ncols = 10\n",
    "nrows = 4\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[15, 6.5])\n",
    "for j in np.arange(ncols):\n",
    "    for i in np.arange(nrows):\n",
    "        ax[i,j].imshow(faces[i*ncols + j].reshape(64,64), cmap=\"gray\")\n",
    "        #if i == 0:\n",
    "        #    ax[i,j].set_title(\"label$=$%s\" % y[idx_subsampled[i]], fontsize=14) \n",
    "        # ax[i].legend()\n",
    "plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "plt.savefig('Faces_ex1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and dictionary images (principal components)\n",
    "\n",
    "X0 = faces.T\n",
    "pca = PCA(n_components=24)\n",
    "pca.fit(X0.T)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "\n",
    "display_dictionary(W, score=s, save_name = \"Faces_PCA_ex1.pdf\", grid_shape=[2,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable nonnegativity constraints \n",
    "\n",
    "X0 = faces.T\n",
    "#X0 /= 100 * np.linalg.norm(X0)\n",
    "\n",
    "n_iter = 200\n",
    "W_list = []\n",
    "\n",
    "nonnegativitiy = [[False, False], [False, True], [True, True]]\n",
    "\n",
    "for i in np.arange(3): \n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    \n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, label_list = label_list, save_name = \"Face_NMF_ex1.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 200\n",
    "W_list = []\n",
    "H_list = []\n",
    "\n",
    "X0 = faces.T\n",
    "#X0 /= 100 * np.linalg.norm(X0)\n",
    "\n",
    "nonnegativitiy = ['PCA', [False, False], [False, True], [True, True]]\n",
    "\n",
    "#PCA \n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X0.T)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "W_list.append(W)\n",
    "H_list.append(s)\n",
    "\n",
    "# MF\n",
    "for i in np.arange(1,len(nonnegativitiy)): \n",
    "    print('!!! nonnegativitiy[i]', nonnegativitiy[i])\n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               W_nonnegativity=nonnegativitiy[i][0],\n",
    "               H_nonnegativity=nonnegativitiy[i][1],\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    H_list.append(H)\n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        label = nonnegativitiy[0]\n",
    "    else:\n",
    "        label = \"W_nonnegativity = %s\" % nonnegativitiy[i][0] + \"\\n\" + \"H_nonnegativity = %s\" % nonnegativitiy[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "score_list = []\n",
    "for i in np.arange(len(nonnegativitiy)):\n",
    "    if i == 0:\n",
    "        score_list.append(H_list[0])\n",
    "    else:\n",
    "        H = H_list[i]\n",
    "        score = np.sum(abs(H), axis=1) # sum of the coefficients of each columns of W = overall usage \n",
    "        score_list.append(score)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, \n",
    "                        label_list = label_list, \n",
    "                        score_list = score_list,\n",
    "                        save_name = \"Faces_PCA_NMF_ex1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable regularizer for W\n",
    "\n",
    "X0 = faces.T\n",
    "print('X0.shape', X0.shape)\n",
    "\n",
    "n_iter = 200\n",
    "W_list = []\n",
    "\n",
    "W_sparsity = [[0, 0], [0.5, 0], [0, 3]]\n",
    "\n",
    "for i in np.arange(3): \n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               a1 = W_sparsity[i][0], # L1 regularizer for W\n",
    "               a12 = W_sparsity[i][1], # L2 regularizer for W\n",
    "               W_nonnegativity=True,\n",
    "               H_nonnegativity=True,\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    \n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(W_sparsity)):\n",
    "    label = \"W_$L_{1}$-regularizer = %.2f\" % W_sparsity[i][0] + \"\\n\" + \"W_$L_{2}$-regularizer = %.2f\" % W_sparsity[i][1]\n",
    "    label_list.append(label)\n",
    "\n",
    "display_dictionary_list(W_list=W_list, label_list = label_list, save_name = \"Face_NMF_ex2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 200\n",
    "W_list = []\n",
    "H_list = []\n",
    "\n",
    "X0 = faces.T\n",
    "#X0 /= 100 * np.linalg.norm(X0)\n",
    "\n",
    "W_sparsity = ['PCA', [0, 0], [0.5, 0], [0, 3]]\n",
    "\n",
    "#PCA \n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X0.T)\n",
    "W = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "W_list.append(W)\n",
    "H_list.append(s)\n",
    "\n",
    "# MF\n",
    "for i in np.arange(1,len(nonnegativitiy)): \n",
    "    print('!!! nonnegativitiy[i]', nonnegativitiy[i])\n",
    "    W, H = ALS(X=X0, \n",
    "               n_components=25, \n",
    "               n_iter=n_iter, \n",
    "               subsample_ratio=1, \n",
    "               a1 = W_sparsity[i][0], # L1 regularizer for W\n",
    "               a12 = W_sparsity[i][1], # L2 regularizer for W\n",
    "               W_nonnegativity=True,\n",
    "               H_nonnegativity=True,\n",
    "               compute_recons_error=True)\n",
    "    W_list.append(W)\n",
    "    H_list.append(H)\n",
    "\n",
    "\n",
    "label_list = []\n",
    "for i in np.arange(len(W_sparsity)):\n",
    "    if i == 0:\n",
    "        label = nonnegativitiy[0]\n",
    "    else:\n",
    "        label = \"W_$L_{1}$-regularizer = %.2f\" % W_sparsity[i][0] + \"\\n\" + \"W_$L_{2}$-regularizer = %.2f\" % W_sparsity[i][1]\n",
    "    label_list.append(label)\n",
    "    \n",
    "score_list = []\n",
    "for i in np.arange(len(W_sparsity)):\n",
    "    if i == 0:\n",
    "        score_list.append(H_list[0])\n",
    "    else:\n",
    "        H = H_list[i]\n",
    "        score = np.sum(abs(H), axis=1) # sum of the coefficients of each columns of W = overall usage \n",
    "        score_list.append(score)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, \n",
    "                        label_list = label_list, \n",
    "                        score_list = score_list,\n",
    "                        save_name = \"Faces_PCA_NMF_ex2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling for 20Newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2onehot(y, list_classes):\n",
    "    \"\"\"\n",
    "    y = list of class lables of length n\n",
    "    output = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    \"\"\"\n",
    "    Y = np.zeros(shape = [len(y), len(list_classes)], dtype=int)\n",
    "    for i in np.arange(Y.shape[0]):\n",
    "        for j in np.arange(len(list_classes)):\n",
    "            if y[i] == list_classes[j]:\n",
    "                Y[i,j] = 1\n",
    "    return Y\n",
    "\n",
    "def onehot2list(y, list_classes=None):\n",
    "    \"\"\"\n",
    "    y = n x k array, i th row = one-hot encoding of y[i] (e.g., [0,0,1,0,0])\n",
    "    output =  list of class lables of length n\n",
    "    \"\"\"\n",
    "    if list_classes is None:\n",
    "        list_classes = np.arange(y.shape[1])\n",
    "\n",
    "    y_list = []\n",
    "    for i in np.arange(y.shape[0]):\n",
    "        idx = np.where(y[i,:]==1)\n",
    "        idx = idx[0][0]\n",
    "        y_list.append(list_classes[idx])\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ('headers','footers','quotes')\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(['thanks','edu','also','would','one','could','please','really','many','anyone','good','right','get','even','want','must','something','well','much','still','said','stay','away','first','looking','things','try','take','look','make','may','include','thing','like','two','or','etc','phone','oh','email'])\n",
    "\n",
    "categories = [\n",
    " 'comp.graphics',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'misc.forsale',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.religion.misc'\n",
    " ]\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_labels = newsgroups_train.target\n",
    "# remove numbers\n",
    "data_cleaned = [re.sub(r'\\d+','', file) for file in newsgroups_train.data]\n",
    "\n",
    "# print 10 random documents\n",
    "#for i in np.arange(10):\n",
    "#    idx = np.random.choice(len(data_cleaned))\n",
    "#    print('>>>> %i th doc \\n\\n %s \\n\\n' % (idx, data_cleaned[idx]))\n",
    "    \n",
    "print('len(newsgroups_labels)', len(newsgroups_labels))\n",
    "print('newsgroups_labels', newsgroups_labels)\n",
    "\n",
    "print('data_cleaned[1]', data_cleaned[1])\n",
    "print('newsgroups_labels[1]', newsgroups_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "vectorizer_BOW = CountVectorizer(stop_words=stopwords_list)\n",
    "vectors_BOW = vectorizer_BOW.fit_transform(data_cleaned).transpose()  # words x docs  # in the form of sparse matrix\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "vectors = vectorizer.fit_transform(data_cleaned).transpose()  # words x docs  # in the form of sparse matrix\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())   # list of words that corresponds to feature coordinates\n",
    "\n",
    "print('>>>> vectors.shape', vectors.shape)\n",
    "i = 4257\n",
    "print('newsgroups_labels[i]', newsgroups_labels[i])\n",
    "print('>>>> data_cleaned[i]', data_cleaned[i])\n",
    "# print('>>>> vectors[:,i] \\n', vectors[:,i])\n",
    "\n",
    "\n",
    "a = vectors[:,i].todense() \n",
    "I = np.where(a>0)\n",
    "\n",
    "count_list = []\n",
    "word_list = []\n",
    "\n",
    "for j in np.arange(len(I[0])):\n",
    "    # idx = np.random.choice(I[0])\n",
    "    idx = I[0][j]\n",
    "    # print('>>>> %i th coordinate <===> %s, count %i' % (idx, idx_to_word[idx], vectors[idx, i]))\n",
    "    count_list.append([idx, vectors_BOW[idx, i], vectors[idx, i]])\n",
    "    word_list.append(idx_to_word[idx])\n",
    "\n",
    "d = pd.DataFrame(data=np.asarray(count_list).T, columns=word_list).T\n",
    "d.columns = ['Coordinate', 'Bag-of-words', 'tf-idf']\n",
    "cols = ['Coordinate', 'Bag-of-words']\n",
    "d[cols] = d[cols].applymap(np.int64)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multiclass_20NEWS(list_classes=[0, 1], full_data=None, vectorizer = 'tf-idf', verbose=True):\n",
    "    # get train and test set from 20NewsGroups of given categories\n",
    "    # vectorizer \\in ['tf-idf', 'bag-of-words']\n",
    "    # documents are loaded up from the following 10 categories\n",
    "    categories = [\n",
    "     'comp.graphics',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'misc.forsale',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'sci.med',\n",
    "     'sci.space',\n",
    "     'talk.politics.guns',\n",
    "     'talk.politics.mideast',\n",
    "     'talk.religion.misc'\n",
    "     ]\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict.update({'categories': categories})\n",
    "    \n",
    "    if full_data is None:\n",
    "        remove = ('headers','footers','quotes')\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        stopwords_list.extend(['thanks','edu','also','would','one','could','please','really','many','anyone','good','right','get','even','want','must','something','well','much','still','said','stay','away','first','looking','things','try','take','look','make','may','include','thing','like','two','or','etc','phone','oh','email'])  \n",
    "        \n",
    "        newsgroups_train_full = fetch_20newsgroups(subset='train', categories=categories, remove=remove) # raw documents\n",
    "        newsgroups_train = [re.sub(r'\\d+','', file) for file in newsgroups_train_full.data] # remove numbers (we are only interested in words)\n",
    "        y = newsgroups_train_full.target # document class labels \n",
    "        Y = list2onehot(y.tolist(), list_classes)\n",
    "       \n",
    "        \n",
    "        if vectorizer == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(stop_words=stopwords_list) \n",
    "        else:\n",
    "            vectorizer = CountVectorizer(stop_words=stopwords_list) \n",
    "            \n",
    "        X = vectorizer.fit_transform(newsgroups_train) # words x docs  # in the form of sparse matrix\n",
    "        X = np.asarray(X.todense())\n",
    "        print('!! X.shape', X.shape)\n",
    "        idx2word = np.array(vectorizer.get_feature_names())   # list of words that corresponds to feature coordinates\n",
    "\n",
    "        data_dict.update({'newsgroups_train': data_cleaned})\n",
    "        data_dict.update({'newsgroups_labels': y})\n",
    "        data_dict.update({'feature_matrix': vectors})\n",
    "        data_dict.update({'idx2word': idx2word})\n",
    "        \n",
    "    else:\n",
    "        X, y = full_data\n",
    "\n",
    "    idx = [i for i in np.arange(len(y)) if y[i] in list_classes] # list of indices where the label y is in list_classes\n",
    "\n",
    "    X01 = X[idx,:]\n",
    "    Y01 = Y[idx,:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_test = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "    y_train = [] # list of one-hot encodings (indicator vectors) of each label  \n",
    "\n",
    "    for i in np.arange(X01.shape[0]):\n",
    "        # for each example i, make it into train set with probabiliy 0.8 and into test set otherwise \n",
    "        U = np.random.rand() # Uniform([0,1]) variable\n",
    "        if U<0.8:\n",
    "            X_train.append(X01[i,:])\n",
    "            y_train.append(Y01[i,:].copy())\n",
    "        else:\n",
    "            X_test.append(X01[i,:])\n",
    "            y_test.append(Y01[i,:].copy())\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    data_dict.update({'X_train': X_train})\n",
    "    data_dict.update({'X_test': X_test})\n",
    "    data_dict.update({'y_train': y_train})\n",
    "    data_dict.update({'y_test': y_test})\n",
    "  \n",
    "    return X_train, X_test, y_train, y_test, data_dict \n",
    "\n",
    "# test \n",
    "X_train, X_test, y_train, y_test, data_dict = sample_multiclass_20NEWS(list_classes=[0, 1, 2,3,4,5,6,7,8,9], \n",
    "                                                                       vectorizer = 'tf-idf',\n",
    "                                                                       full_data=None)\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_test.shape', y_test.shape)\n",
    "print('y_test', y_test)\n",
    "#print('y_list', onehot2list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = data_dict.get('idx2word')\n",
    "categories = data_dict.get('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "\n",
    "def plot_topic_wordcloud(W, idx2word, num_keywords_in_topic=5, save_name=None, grid_shape = [2,5]):\n",
    "        # plot the class-conditioanl PMF as wordclouds \n",
    "        # W = (p x r) (words x topic)\n",
    "        # idx2words = list of words used in the vectorization of documents \n",
    "        # categories = list of class labels\n",
    "        # prior on class labels = empirical PMF = [ # class i examples / total ]\n",
    "        # class-conditional for class i = [ # word j in class i examples / # words in class i examples]\n",
    "             \n",
    "        fig, axs = plt.subplots(nrows=grid_shape[0], ncols=grid_shape[1], figsize=(15, 6), subplot_kw={'xticks': [], 'yticks': []})\n",
    "        for ax, i in zip(axs.flat, np.arange(W.shape[1])):\n",
    "            # dist = W[:,i]/np.sum(W[:,i])\n",
    "\n",
    "            ### Take top k keywords in each topic (top k coordinates in each column of W)\n",
    "            ### to generate text data corresponding to the ith topic, and then generate its wordcloud\n",
    "            list_words = []\n",
    "        \n",
    "            idx = np.argsort(W[:,i])\n",
    "            idx = np.flip(idx)    \n",
    "           \n",
    "            for j in range(num_keywords_in_topic):\n",
    "                list_words.append(idx2word[idx[j]])\n",
    "                \n",
    "            Y = \" \".join(list_words)\n",
    "            #stopwords = STOPWORDS\n",
    "            #stopwords.update([\"’\", \"“\", \"”\", \"000\", \"000 000\", \"https\", \"co\", \"19\", \"2019\", \"coronavirus\",\n",
    "            #                  \"virus\", \"corona\", \"covid\", \"ncov\", \"covid19\", \"amp\"])\n",
    "            wc = WordCloud(background_color=\"black\",\n",
    "                                  relative_scaling=0,\n",
    "                                  width=400,\n",
    "                                  height=400).generate(Y)\n",
    "            \n",
    "            ax.imshow(wc.recolor(color_func=grey_color_func, random_state=3),\n",
    "                                 interpolation=\"bilinear\")\n",
    "            \n",
    "            # ax.set_xlabel(categories[i], fontsize='20')\n",
    "            # ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.08)\n",
    "        if save_name is not None:\n",
    "            plt.savefig(save_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X_train.T\n",
    "\n",
    "print('X0.shape', X0.shape)\n",
    "\n",
    "W, H = ALS(X=X0, \n",
    "           n_components=10, \n",
    "           n_iter=20, \n",
    "           subsample_ratio=1, \n",
    "           a1 = 0, # L1 regularizer for W\n",
    "           a12 = 0, # L2 regularizer for W\n",
    "           W_nonnegativity=True,\n",
    "           H_nonnegativity=True,\n",
    "           compute_recons_error=True)\n",
    "\n",
    "plot_topic_wordcloud(W, idx2word=idx2word, num_keywords_in_topic=7, grid_shape=[2,5], save_name=\"20NEWS_topic1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling by NMF\n",
    "X0 = X_train.T\n",
    "\n",
    "W, H = ALS(X=X0, \n",
    "           n_components=10, \n",
    "           n_iter=20, \n",
    "           subsample_ratio=1, \n",
    "           a1 = 0, # L1 regularizer for W\n",
    "           a12 = 0, # L2 regularizer for W\n",
    "           W_nonnegativity=True,\n",
    "           H_nonnegativity=False,\n",
    "           compute_recons_error=True)\n",
    "\n",
    "\n",
    "plot_topic_wordcloud(W, idx2word=idx2word, num_keywords_in_topic=7, grid_shape = [2,5], save_name=\"20NEWS_topic2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt Orthogonalization of a given matrix\n",
    "\n",
    "def orthogonalize(U, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Orthogonalizes the matrix U (d x n) using Gram-Schmidt Orthogonalization.\n",
    "    If the columns of U are linearly dependent with rank(U) = r, the last n-r columns \n",
    "    will be 0.\n",
    "    \n",
    "    Args:\n",
    "        U (numpy.array): A d x n matrix with columns that need to be orthogonalized.\n",
    "        eps (float): Threshold value below which numbers are regarded as 0 (default=1e-15).\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.array): A d x n orthogonal matrix. If the input matrix U's cols were\n",
    "            not linearly independent, then the last n-r cols are zeros.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(U[0])\n",
    "    # numpy can readily reference rows using indices, but referencing full rows is a little\n",
    "    # dirty. So, work with transpose(U)\n",
    "    V = U.T\n",
    "    for i in range(n):\n",
    "        prev_basis = V[0:i]     # orthonormal basis before V[i]\n",
    "        coeff_vec = np.dot(prev_basis, V[i].T)  # each entry is np.dot(V[j], V[i]) for all j < i\n",
    "        # subtract projections of V[i] onto already determined basis V[0:i]\n",
    "        V[i] -= np.dot(coeff_vec, prev_basis).T\n",
    "        if np.linalg.norm(V[i]) < eps:\n",
    "            V[i][V[i] < eps] = 0.   # set the small entries to 0\n",
    "        else:\n",
    "            V[i] /= np.linalg.norm(V[i])\n",
    "    return V.T\n",
    "\n",
    "# Example:\n",
    "A = np.random.rand(2,2)\n",
    "print('A \\n', A)\n",
    "print('orthogonalize(A) \\n', orthogonalize(A))\n",
    "print('A.T @ A \\n', A.T @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_PCA(X,\n",
    "        n_components = 10, # number of columns in the dictionary matrix W\n",
    "        n_iter=10,\n",
    "        W_ini=None,\n",
    "        subsample_ratio=1,\n",
    "        n_workers = 1):\n",
    "    \n",
    "        '''\n",
    "        Given data matrix X of shape (d x n), compute its rank r=n_components PCA:\n",
    "            \\hat{W} = \\argmax_{W} var(Proj_{W}(X)) \n",
    "                    = \\argmin_{W} || X - Proj_{W}(X) ||_{F}^{2}\n",
    "        where W is an (d x r) matrix of rank r. \n",
    "        '''\n",
    "        \n",
    "        d, n = X.shape\n",
    "        r = n_components\n",
    "        \n",
    "        X_mean = np.mean(X, axis=1).reshape(-1,1)\n",
    "        X_centered = X - np.repeat(X_mean, X0.shape[1], axis=1)\n",
    "        print('subsample_size:', n//subsample_ratio)\n",
    "        \n",
    "        # Initialize factors \n",
    "        W_list = []\n",
    "        loss_list = []\n",
    "        \n",
    "        \n",
    "        for i in trange(n_workers):\n",
    "            W = np.random.rand(d,r)\n",
    "            if W_ini is not None:\n",
    "                W = W_ini\n",
    "                \n",
    "            A = np.zeros(shape=[r, n//subsample_ratio]) # aggregate matrix for code H\n",
    "          \n",
    "            # Perform EM updates\n",
    "            for j in np.arange(n_iter):\n",
    "                idx_data = np.random.choice(np.arange(X.shape[1]), X.shape[1]//subsample_ratio, replace=False)\n",
    "                X1 = X_centered[:,idx_data]\n",
    "                H = np.linalg.inv(W.T @ W) @ (W.T @ X1) # E-step\n",
    "                # A = (1-(1/(j+1)))*A + (1/(j+1))*H # Aggregation \n",
    "                W = X1 @ H.T @ np.linalg.inv(H @ H.T) # M-step\n",
    "                # W = X1 @ A.T @ np.linalg.inv(A @ A.T) # M-step\n",
    "            \n",
    "                # W = orthogonalize(W)\n",
    "                #if compute_recons_error and (j > n_iter-2) :   \n",
    "                #    print('iteration %i, reconstruction error %f' % (j, np.linalg.norm(X_centered-W@(W.T @ X_centered))))\n",
    "\n",
    "            W_list.append(W.copy())\n",
    "            loss_list.append(np.linalg.norm(X_centered-W@(W.T @ X_centered)))\n",
    "        \n",
    "        idx = np.argsort(loss_list)[0]\n",
    "        W = W_list[idx]\n",
    "        print('loss_list',np.asarray(loss_list)[np.argsort(loss_list)])\n",
    "        \n",
    "        return orthogonalize(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Olivetti Face dataset\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True,\n",
    "                                random_state=np.random.seed(0))\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "#faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "#faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "print(\"faces_centered.shape\", faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM_PCA and dictionary images (principal components)\n",
    "\n",
    "X0 = faces.T\n",
    "W = EM_PCA(X0, W_ini = None, n_workers=10, n_iter=200, subsample_ratio=2, n_components=24)\n",
    "\n",
    "display_dictionary(W, score=None, save_name = \"Faces_EM_PCA_ex1.pdf\", grid_shape=[2,12])\n",
    "\n",
    "cov = np.cov(X0)\n",
    "print('(cov @ W)[:,0] / W[:,0]', (cov @ W)[:,0] / W0[:,0])\n",
    "print('var coeff', np.std((cov @ W)[:,0] / W[:,0]))\n",
    "print('var coeff exact', np.std((cov @ W0)[:,0] / W0[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coefficients of Cov @ W / W for exact PCA and EM PCA\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "pca.fit(X0.T)\n",
    "W0 = pca.components_.T\n",
    "\n",
    "axs[0].plot((cov @ W0)[:,0] / W0[:,0], label='Exact PCA, 1st comp.')\n",
    "axs[0].legend(fontsize=13)\n",
    "axs[1].plot((cov @ W)[:,0] / W[:,0], label='EM PCA, 1st comp.')\n",
    "axs[1].legend(fontsize=13)\n",
    "plt.savefig(\"EM_PCA_coeff_plot1.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = faces.T\n",
    "pca = PCA(n_components=24)\n",
    "pca.fit(X0.T)\n",
    "W0 = pca.components_.T\n",
    "s = pca.singular_values_\n",
    "cov = np.cov(X0)\n",
    "print('(cov @ W)[:,0] / W[:,0]', (cov @ W0)[:,0] / W0[:,0])\n",
    "\n",
    "display_dictionary(W0, score=s, save_name = \"Faces_PCA_ex1.pdf\", grid_shape=[2,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.sum(X0, axis=1).reshape(-1,1)/X0.shape[1]\n",
    "X_centered = X0 - np.repeat(X_mean, X0.shape[1], axis=1)\n",
    "Cov = (X_centered @ X_centered.T) / X0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Cov @ W)[:,0] / W[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(X0)\n",
    "(cov @ W0)[:,0] / W0[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(eig_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.real(eig_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "        [0.387,4878, 5.42],\n",
    "        [0.723,12104,5.25],\n",
    "        [1,12756,5.52],\n",
    "        [1.524,6787,3.94],\n",
    "    ])\n",
    "\n",
    "#centering the data\n",
    "x0 =  x - np.mean(x, axis = 0)  \n",
    "\n",
    "cov = np.cov(x0, rowvar = False)\n",
    "print('cov', cov)\n",
    "print('cov', np.cov(x, rowvar = False))\n",
    "\n",
    "evals , evecs = np.linalg.eigh(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_NMF(X,\n",
    "           n_components = 10, # number of columns in the dictionary matrix W\n",
    "           n_iter=100,\n",
    "           H_nonnegativity=True,\n",
    "           W_nonnegativity=True,\n",
    "           compute_recons_error=False,\n",
    "           subsample_ratio = 10):\n",
    "    \n",
    "        '''\n",
    "        Given data matrix X, use Gradient Descent to find factors W,H so that \n",
    "                                || X - WH ||_{F}^2 + a0*|H|_{1} + a1*|W|_{1} + a12 * |W|_{F}^{2}\n",
    "        is minimized (at least locally)\n",
    "        '''\n",
    "        \n",
    "        d, n = X.shape\n",
    "        r = n_components\n",
    "        \n",
    "        #normalization = np.linalg.norm(X.reshape(-1,1),1)/np.product(X.shape) # avg entry of X\n",
    "        #print('!!! avg entry of X', normalization)\n",
    "        #X = X/normalization\n",
    "\n",
    "        # Initialize factors \n",
    "        W = np.random.rand(d,r)\n",
    "        H = np.random.rand(r,n) \n",
    "    \n",
    "        for i in trange(n_iter):\n",
    "            W1 = W.copy()\n",
    "            H1 = H.copy()\n",
    "            \n",
    "            stepsize = 1/((np.trace(W1.T @ W1)  + 1)) # step size given by the sum of singular values of W\n",
    "            H -= stepsize * W1.T @ (W1 @ H1-X)\n",
    "            \n",
    "            stepsize = 1/((np.trace(H1 @ H1.T)  + 1)) # step size given by the sum of singular values of H\n",
    "            W -= stepsize * (W1 @ H1 - X)@ H1.T  \n",
    "            \n",
    "            # Q: Uniform step size for both W and H seems not to converge well. Why?\n",
    "            \n",
    "            if H_nonnegativity:\n",
    "                H = np.maximum(H,0)              \n",
    "            \n",
    "            if W_nonnegativity:\n",
    "                W = np.maximum(W,0)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \"\"\"\n",
    "            stepsize = 1/((np.trace(W.T @ W)  + 1))\n",
    "            H -= stepsize * W1.T @ (W1 @ H1-X)\n",
    "            if H_nonnegativity:\n",
    "                H = np.maximum(H,0)              \n",
    "            \n",
    "            stepsize = 1/((np.trace(H @ H.T)  + 1))\n",
    "            W -= stepsize * (W1 @ H1 - X)@ H1.T       \n",
    "            if W_nonnegativity:\n",
    "                W = np.maximum(W,0)\n",
    "            \"\"\"\n",
    "            \n",
    "            if compute_recons_error and (i % 10 == 0) :\n",
    "                print('iteration %i, reconstruction error %f' % (i, np.linalg.norm(X-W@H)**2))\n",
    "        return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Data and its factorization \n",
    "\n",
    "W0 = np.random.rand(10,5)\n",
    "H0 = np.random.rand(5,20)\n",
    "X0 = W0 @ H0\n",
    "\n",
    "W, H = GD_NMF(X=X0, \n",
    "           n_components=5, \n",
    "           n_iter=500, \n",
    "           H_nonnegativity=True,\n",
    "           W_nonnegativity=True,\n",
    "           compute_recons_error=True,\n",
    "           subsample_ratio=10)\n",
    "\n",
    "print('reconstruction error (relative) = %f' % (np.linalg.norm(X0-W@H)**2/np.linalg.norm(X0)**2))\n",
    "print('Dictionary error (relative) = %f' % (np.linalg.norm(W0 - W)**2/np.linalg.norm(W0)**2))\n",
    "print('Code error (relative) = %f' % (np.linalg.norm(H0-H)**2/np.linalg.norm(H0)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True,\n",
    "                                random_state=np.random.seed(0))\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "#faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "#faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "print(\"faces_centered.shape\", faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable regularizer for W\n",
    "\n",
    "X0 = faces.T\n",
    "print('X0.shape', X0.shape)\n",
    "\n",
    "n_iter = 500\n",
    "W_list = []\n",
    "\n",
    "W, H = GD_NMF(X=X0, \n",
    "           n_components=25, \n",
    "           n_iter=n_iter, \n",
    "           subsample_ratio=1, \n",
    "           W_nonnegativity=True,\n",
    "           H_nonnegativity=True,\n",
    "           compute_recons_error=True)\n",
    "W_list.append(W)\n",
    "    \n",
    "display_dictionary_list(W_list=W_list, label_list = label_list, save_name = \"Face_NMF_GD_sample1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colourgraphenv",
   "language": "python",
   "name": "colourgraphenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
